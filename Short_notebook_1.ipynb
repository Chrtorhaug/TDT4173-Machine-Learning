{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d318c3-43e5-413f-a80e-f6e990d95179",
   "metadata": {},
   "source": [
    "##TEAM MEMBERS\n",
    "\n",
    "Aidan Stautland, 502841; Christian Torhaug, 564355; Jens Skaug, STUDID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11a85d1-1da9-4b7c-a925-3a1b682388d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795c0d5-96b8-4bda-95df-a7d0df3697db",
   "metadata": {},
   "source": [
    "ais_sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a4fcd7-0fa8-4866-ae47-b7666655e343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampledf = pd.read_csv(\"ais_sample_submission.csv\")\n",
    "\n",
    "sampledf[\"ID\"] = sampledf[\"ID\"].apply(lambda x: int(x))\n",
    "sampledf[\"latitude_predicted\"] = sampledf[\"latitude_predicted\"].apply(lambda x: float(x))\n",
    "sampledf[\"longitude_predicted\"] = sampledf[\"longitude_predicted\"].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd046a4-909a-43b6-b859-acf7fc2a3532",
   "metadata": {
    "tags": []
   },
   "source": [
    "ais_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57fb181c-5054-42eb-ad17-7c1636e87dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testdf = pd.read_csv(\"ais_test.csv\")\n",
    "\n",
    "testdf[\"time\"] = testdf[\"time\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
    "testdf[\"scaling_factor\"] = testdf[\"scaling_factor\"].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904c6f1-ce80-42d0-ac0a-e9e06e45078b",
   "metadata": {},
   "source": [
    "ais_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10076f61-fb54-4eaf-b770-08b532f40609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(\"~/Ais_Data/ais_train.csv\", sep=\"|\")\n",
    "\n",
    "def parseEta(row):\n",
    "    try:\n",
    "        return datetime.strptime(row['etaRaw'], \"%m-%d %H:%M\")\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "#traindf[\"time\"] = traindf[\"time\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
    "traindf[\"time\"] = pd.to_datetime(traindf[\"time\"]) \n",
    "traindf[\"cog\"] = traindf[\"cog\"].apply(lambda x: float(x) if float(x) < 360.0 else 360.0)\n",
    "traindf[\"sog\"] = traindf[\"sog\"].apply(lambda x: float(x) if float(x) <= 30.0 else 0)\n",
    "traindf[\"rot\"] = traindf[\"rot\"].apply(lambda x: int(x))\n",
    "traindf[\"heading\"] = traindf[\"heading\"].apply(lambda x: int(x) if int(x) < 360.0 else 360.0)\n",
    "traindf[\"etaRaw\"] = traindf.apply(parseEta, axis=1)\n",
    "traindf[\"latitude\"] = traindf[\"latitude\"].apply(lambda x: float(x))\n",
    "traindf[\"longitude\"] = traindf[\"longitude\"].apply(lambda x: float(x))\n",
    "\n",
    "traindf[\"navstat\"] = traindf[\"navstat\"].apply(lambda x: int(x) if x in [0, 1, 5, 8] else 15)\n",
    "traindf[\"navstat\"] = traindf[\"navstat\"].replace(8, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1a308-b414-4c80-8dfa-5f241745525b",
   "metadata": {},
   "source": [
    "ports.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300bcee3-3599-4bc4-ba72-d727fd57a984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "portsdf = pd.read_csv(\"ports.csv\", sep=\"|\")\n",
    "\n",
    "portsdf[\"latitude\"] = portsdf[\"latitude\"].apply(lambda x: float(x))\n",
    "portsdf[\"longitude\"] = portsdf[\"longitude\"].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a5db6-3378-4561-94a1-caa76c01f806",
   "metadata": {},
   "source": [
    "schedules_to_may_2024.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c88489f0-e85d-4015-9799-d6856c59b518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schedulesdf = pd.read_csv(\"~/Ais_Data/schedules_to_may_2024.csv\", sep=\"|\")\n",
    "schedulesdf = schedulesdf.drop_duplicates()\n",
    "schedulesdf = schedulesdf.dropna(subset=['portName'])\n",
    "\n",
    "schedulesdf[\"arrivalDate\"] = pd.to_datetime(schedulesdf[\"arrivalDate\"], errors='coerce')\n",
    "schedulesdf[\"arrivalDate\"] = schedulesdf[\"arrivalDate\"].dt.tz_convert('UTC').dt.tz_localize(None)\n",
    "schedulesdf[\"sailingDate\"] = pd.to_datetime(schedulesdf[\"sailingDate\"], errors='coerce')\n",
    "schedulesdf[\"sailingDate\"] = schedulesdf[\"sailingDate\"].dt.tz_convert('UTC').dt.tz_localize(None)\n",
    "schedulesdf[\"portLatitude\"] = schedulesdf[\"portLatitude\"].apply(lambda x: float(x))\n",
    "schedulesdf[\"portLongitude\"] = schedulesdf[\"portLongitude\"].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff9b5b2-d1d1-4bc0-98d0-452f589c1150",
   "metadata": {
    "tags": []
   },
   "source": [
    "vessels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d2444d-748d-4bfe-90d4-65ab5e748596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vehiclesdf = pd.read_csv(\"vessels.csv\", sep=\"|\")\n",
    "\n",
    "vehiclesdf[\"CEU\"] = vehiclesdf[\"CEU\"].apply(lambda x: int(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"DWT\"] = vehiclesdf[\"DWT\"].apply(lambda x: int(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"GT\"] = vehiclesdf[\"GT\"].apply(lambda x: int(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"NT\"] = vehiclesdf[\"NT\"].apply(lambda x: int(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"vesselType\"] = vehiclesdf[\"vesselType\"].apply(lambda x: int(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"breadth\"] = vehiclesdf[\"breadth\"].apply(lambda x: float(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"depth\"] = vehiclesdf[\"depth\"].apply(lambda x: float(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"draft\"] = vehiclesdf[\"draft\"].apply(lambda x: float(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"enginePower\"] = vehiclesdf[\"enginePower\"].apply(lambda x: float(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"freshWater\"] = vehiclesdf[\"freshWater\"].apply(lambda x: float(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"fuel\"] = vehiclesdf[\"fuel\"].apply(lambda x: float(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"length\"] = vehiclesdf[\"length\"].apply(lambda x: float(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"maxHeight\"] = vehiclesdf[\"maxHeight\"].apply(lambda x: float(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"maxSpeed\"] = vehiclesdf[\"maxSpeed\"].apply(lambda x: float(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"maxWidth\"] = vehiclesdf[\"maxWidth\"].apply(lambda x: float(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"rampCapacity\"] = vehiclesdf[\"rampCapacity\"].apply(lambda x: float(x) if pd.notna(x) else 0)\n",
    "vehiclesdf[\"yearBuilt\"] = vehiclesdf[\"yearBuilt\"].apply(lambda x: datetime.strptime(str(x), \"%Y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5105661-a278-4644-8f38-68ea007b3627",
   "metadata": {},
   "source": [
    "Function for calculating time difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34db8edf-203d-415b-afc7-3cd425e4dda4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def timeDiff(t1: datetime, t2: datetime, i=\"s\"):\n",
    "    diff = t1 - t2 if t1 >= t2  else t2 - t1\n",
    "    if i == \"s\":\n",
    "        return diff.total_seconds()\n",
    "    elif i == \"m\":\n",
    "        return diff.total_seconds() // 60\n",
    "    elif i == \"h\":\n",
    "        return diff.total_seconds() // 3600\n",
    "    elif i == \"d\":\n",
    "        return diff.days\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dce401-48f3-416d-bfdf-3b0b3bdd2222",
   "metadata": {
    "tags": []
   },
   "source": [
    "Adding WORKING_SIZE previous nodes to the ais_train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5744bef-e36d-4eba-99a2-3bd08be69135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prev_nodes_train(traindf: pd.DataFrame, WINDOW_SIZE=1):\n",
    "    for j in range(1, WINDOW_SIZE + 1):\n",
    "        traindf[f'latitude_{j}'] = traindf.groupby('vesselId')['latitude'].shift(j)\n",
    "        traindf[f'longitude_{j}'] = traindf.groupby('vesselId')['longitude'].shift(j)\n",
    "        traindf[f'cog_{j}'] = traindf.groupby('vesselId')['cog'].shift(j)\n",
    "        traindf[f'sog_{j}'] = traindf.groupby('vesselId')['sog'].shift(j)\n",
    "        \n",
    "        traindf[f'time_{j}'] = traindf.groupby('vesselId')['time'].shift(j)\n",
    "        traindf[f'time_{j}'] = (traindf['time'] - traindf[f'time_{j}']).dt.total_seconds()\n",
    "    return traindf\n",
    "\n",
    "def prev_nodes_test(traindf: pd.DataFrame, testdf: pd.DataFrame, WINDOW_SIZE=1):\n",
    "    grouped_train = traindf.groupby('vesselId')\n",
    "    \n",
    "    for j in range(1, WINDOW_SIZE + 1):\n",
    "        testdf[f'latitude_{j}'] = None\n",
    "        testdf[f'longitude_{j}'] = None\n",
    "        testdf[f'cog_{j}'] = None\n",
    "        testdf[f'sog_{j}'] = None\n",
    "        testdf[f'time_{j}'] = None\n",
    "\n",
    "    for vesselId, test_vessel_data in testdf.groupby('vesselId'):\n",
    "        if vesselId in grouped_train.groups:\n",
    "            train_vessel_data = grouped_train.get_group(vesselId)\n",
    "            for j in range(1, WINDOW_SIZE + 1):\n",
    "                if len(train_vessel_data) >= j:\n",
    "                    train_row = train_vessel_data.iloc[-j]\n",
    "                    testdf.loc[test_vessel_data.index, f'latitude_{j}'] = train_row['latitude']\n",
    "                    testdf.loc[test_vessel_data.index, f'longitude_{j}'] = train_row['longitude']\n",
    "                    testdf.loc[test_vessel_data.index, f'cog_{j}'] = train_row['cog']\n",
    "                    testdf.loc[test_vessel_data.index, f'sog_{j}'] = train_row['sog']\n",
    "                    testdf.loc[test_vessel_data.index, f'time_{j}'] = (testdf.loc[test_vessel_data.index, 'time'] - train_row['time']).dt.total_seconds()\n",
    "    return testdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06055e33-ff33-4864-b0ca-124912b8ac4e",
   "metadata": {},
   "source": [
    "Checking if a boat is on a schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "185b1c6a-0e42-4964-8627-75b8e20bc58d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(schedulesdf.columns)\\nif schedulesdf.index.name != \\'vesselId\\':\\n    schedulesdf = schedulesdf.set_index(\\'vesselId\\')\\n\\ndef onSchedule_optimized(testdf: pd.DataFrame, schedulesdf: pd.DataFrame, vesselId: str, time: datetime):\\n    if vesselId in schedulesdf.index:\\n        vessel_schedules = schedulesdf.loc[vesselId]\\n        if isinstance(vessel_schedules, pd.Series):\\n            vessel_schedules = vessel_schedules.to_frame().T\\n            \\n        fitting_schedules = (vessel_schedules[\"sailingDate\"] >= time) & (vessel_schedules[\"arrivalDate\"] <= time)\\n        if fitting_schedules.any():\\n            matching_schedules = vessel_schedules.loc[fitting_schedules]\\n            if len(matching_schedules) == 1:\\n                return matching_schedules.iloc[0]\\n    return None\\nprint(schedulesdf.columns)\\nschedulesdf = schedulesdf.set_index(\\'vesselId\\')\\ntestdf[\"onSchedule\"] = testdf.apply(lambda row: onSchedule_optimized(testdf, schedulesdf, row[\"vesselId\"], row[\"time\"]))\\nprint(testdf.loc[:10])\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def onSchedule(schedulesdf: pd.DataFrame, vesselId: str, time: datetime):\n",
    "    if schedulesdf.index.name != 'vesselId':\n",
    "        schedulesdf = schedulesdf.set_index('vesselId')\n",
    "        \n",
    "    if vesselId in schedulesdf.index:\n",
    "        vessel_schedules = schedulesdf.loc[vesselId]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    fitting_schedules = (vessel_schedules[\"sailingDate\"] >= time) & (vessel_schedules[\"arrivalDate\"] <= time)\n",
    "    if fitting_schedules.any():\n",
    "        matching_schedules = vessel_schedules.loc[fitting_schedules]\n",
    "        if len(matching_schedules) == 1:\n",
    "            return matching_schedules.iloc[0]\n",
    "    return None\n",
    "\"\"\"\n",
    "print(schedulesdf.columns)\n",
    "if schedulesdf.index.name != 'vesselId':\n",
    "    schedulesdf = schedulesdf.set_index('vesselId')\n",
    "\n",
    "def onSchedule_optimized(testdf: pd.DataFrame, schedulesdf: pd.DataFrame, vesselId: str, time: datetime):\n",
    "    if vesselId in schedulesdf.index:\n",
    "        vessel_schedules = schedulesdf.loc[vesselId]\n",
    "        if isinstance(vessel_schedules, pd.Series):\n",
    "            vessel_schedules = vessel_schedules.to_frame().T\n",
    "            \n",
    "        fitting_schedules = (vessel_schedules[\"sailingDate\"] >= time) & (vessel_schedules[\"arrivalDate\"] <= time)\n",
    "        if fitting_schedules.any():\n",
    "            matching_schedules = vessel_schedules.loc[fitting_schedules]\n",
    "            if len(matching_schedules) == 1:\n",
    "                return matching_schedules.iloc[0]\n",
    "    return None\n",
    "print(schedulesdf.columns)\n",
    "schedulesdf = schedulesdf.set_index('vesselId')\n",
    "testdf[\"onSchedule\"] = testdf.apply(lambda row: onSchedule_optimized(testdf, schedulesdf, row[\"vesselId\"], row[\"time\"]))\n",
    "print(testdf.loc[:10])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96633d29-9ce0-4060-9cc9-eff982eea98b",
   "metadata": {},
   "source": [
    "Finding Last Data Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddc2812e-2abf-4619-aacb-938165412480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lastData(testdf: pd.DataFrame, traindf: pd.DataFrame):\n",
    "    vessels = testdf[\"vesselId\"].unique().tolist()\n",
    "    indexes = []\n",
    "\n",
    "    for v in vessels:\n",
    "        vessel_data = traindf[traindf['vesselId'] == v]\n",
    "        indexes.append(vessel_data.sort_values(by=\"time\").iloc[-1].name)\n",
    "\n",
    "    return traindf.loc[indexes].sort_values(by=\"time\")\n",
    "\n",
    "#print(lastData(testdf, traindf).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461f805-8291-48a6-b800-e72ebe159585",
   "metadata": {},
   "source": [
    "Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1618a4fe-3d76-4e4a-ae3e-b518e2064085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def distance(lat1, long1, lat2, long2):\n",
    "    return geodesic((lat1, long1), (lat2, long2)).meters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6668924-93ec-41a1-8fbe-12102f16583d",
   "metadata": {},
   "source": [
    "Checking if a vessel is near a port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93155d30-5d40-4c77-aee4-78936e619b68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "PORT_DIST_THRESH = 3000\n",
    "\n",
    "ports_coords = portsdf[[\"latitude\", \"longitude\"]].values\n",
    "kdtree = KDTree(ports_coords, leaf_size=30)\n",
    "\n",
    "def nearPort(portsdf: pd.DataFrame, lat, long):\n",
    "    rough_dist = PORT_DIST_THRESH / 100000\n",
    "    indices = kdtree.query_radius([[lat, long]], r=rough_dist)\n",
    "    for i in indices[0]:\n",
    "        port = portsdf.iloc[i]\n",
    "        if distance(lat, long ,port[\"latitude\"],port[\"longitude\"]) <= PORT_DIST_THRESH:\n",
    "            return port\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0156e-82df-41f5-91b2-43d66d97cdbd",
   "metadata": {},
   "source": [
    "Checking if a vessel is far from land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f998c34a-0539-466d-a918-c6f3eedcf6c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAP_LAND_PATH = \"~/datasets/ne_10m_land.zip\"\n",
    "MAP_OCEAN_PATH = \"~/datasets/ne_10m_ocean.zip\"\n",
    "    \n",
    "land_world = gpd.read_file(MAP_LAND_PATH)\n",
    "ocean_world = gpd.read_file(MAP_OCEAN_PATH)\n",
    "\n",
    "def farFromCoast(latitude, longitude, land_world=land_world, thresh=12):\n",
    "    land_world_projected = land_world.to_crs(epsg=3857)\n",
    "    point = gpd.GeoSeries([Point(longitude, latitude)], crs=\"EPSG:4326\").to_crs(epsg=3857)\n",
    "    \n",
    "    if land_world_projected.contains(point.iloc[0]).any():\n",
    "        return False\n",
    "    \n",
    "    land_boundary = land_world_projected.geometry.boundary.unary_union\n",
    "    distanceToCoast = point.distance(land_boundary)\n",
    "    distanceNM = distanceToCoast.min() / 1852\n",
    "    \n",
    "    return distanceNM > thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d0344-3441-4f64-89e8-328fe462a8a1",
   "metadata": {},
   "source": [
    "Calculating average SOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4966acc-484f-4743-8b43-1785424e6f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def avgSOG(traindf: pd.DataFrame):\n",
    "    filtered_df = traindf[traindf[\"navstat\"] == 0]\n",
    "    sog_means = filtered_df.groupby(\"vesselId\")[\"sog\"].mean()\n",
    "    return sog_means.to_dict()\n",
    "\n",
    "avg = avgSOG(traindf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e4163-f3e9-44e0-b869-ff5522cdfbbe",
   "metadata": {},
   "source": [
    "Update Navstat for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e17d59b8-43a6-43a4-be00-88df7e6070e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def updateNavStat(traindf: pd.DataFrame, testdf: pd.DataFrame, schedulesdf: pd.DataFrame):\n",
    "    statuses = ['Under_Way', 'Anchored', 'Moored']\n",
    "    for status in statuses:\n",
    "        testdf[status] = False\n",
    "\n",
    "    grouped_train = traindf.groupby('vesselId')\n",
    "    for vesselId, test_vessel_data in testdf.groupby('vesselId'):\n",
    "        if vesselId in grouped_train.groups:\n",
    "            last_train_row = grouped_train.get_group(vesselId).iloc[-1]\n",
    "            for status in statuses:\n",
    "                testdf.loc[test_vessel_data.index, status] = last_train_row[status]\n",
    "                if last_train_row[status] is True:\n",
    "                    testdf.loc[test_vessel_data.index, [s for s in statuses if s != status]] = False\n",
    "        \"\"\"for idx, row in test_vessel_data.iterrows():\n",
    "            if onSchedule(schedulesdf, vesselId, row['time']) is not None:\n",
    "                testdf.loc[idx, 'Moored'] = True\n",
    "                testdf.loc[idx, 'Under_Way'] = False\n",
    "                testdf.loc[idx, 'Anchored'] = False \"\"\"\n",
    "    return testdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a20ca6-4d05-477a-9f61-f3ac18c54a86",
   "metadata": {
    "tags": []
   },
   "source": [
    "Creating Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "851ed9ef-f459-479e-a9a4-209d8214ad73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def submission(sampledf: pd.DataFrame, lat_pred, lon_pred, deliver=False, name=\"SampleTest\") -> pd.DataFrame:\n",
    "    sample = sampledf.copy()\n",
    "\n",
    "    for i in range(len(lat_pred)):\n",
    "        sample.loc[i,\"longitude_predicted\"], sample.loc[i, \"latitude_predicted\"] = lon_pred[i], lat_pred[i]\n",
    "    \n",
    "    if deliver:\n",
    "        sample.to_csv(name + \".csv\", index=False)\n",
    "        \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c3b6f4-8d83-4adf-a05d-014a539a8b4c",
   "metadata": {},
   "source": [
    "Copying Test and Train Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec8cb495-9a9c-4235-b1ca-9394a08b03f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainRFR = traindf.copy()\n",
    "testRFR = testdf.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c6fea4-e22e-4f47-8b2d-32c1c71373de",
   "metadata": {},
   "source": [
    "Splitting data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8c08fa7-4c3d-4535-8a97-f3c0e3040284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vessels = trainRFR[\"vesselId\"].unique().tolist()\n",
    "vessel_dict = {vessel: i for i, vessel in enumerate(vessels)}\n",
    "\n",
    "ports = portsdf[\"portId\"].unique().tolist()\n",
    "port_dict = {port: i for i, port in enumerate(ports)}\n",
    "\n",
    "# Adding previous nodes\n",
    "window_size = 2\n",
    "trainRFR = prev_nodes_train(trainRFR, window_size)\n",
    "testRFR = prev_nodes_test(trainRFR, testRFR, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349114d3-78a0-4202-a0af-aad4445df26c",
   "metadata": {},
   "source": [
    "ML Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9632b87c-dbab-4ae7-a89e-9fda971fe57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model without Feature Engineering\n",
      "Mean Squared Error (MSE): 0.9960\n",
      "Mean Absolute Error (MAE): 0.0589\n",
      "R^2 Score: 0.9981\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#print(X_test.head())\u001b[39;00m\n\u001b[1;32m     44\u001b[0m clf_lon \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[43mclf_lon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_lon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m y_pred_lon \u001b[38;5;241m=\u001b[39m clf_lon\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest Model without Feature Engineering\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate_result(y_test, y_pred):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "    print(f'R^2 Score: {r2:.4f}')\n",
    "\n",
    "y_lat = trainRFR[\"latitude\"].copy()\n",
    "y_lon = trainRFR[\"longitude\"].copy()\n",
    "X = trainRFR.drop(columns=['latitude', 'longitude'])\n",
    "\n",
    "X = X.drop(columns=[\"etaRaw\", \"portId\", \"cog\", \"sog\", \"rot\", \"heading\"])\n",
    "#X[\"time\"] = X[\"time\"].astype(\"int64\") / 10**9\n",
    "X[\"time\"] = pd.to_numeric(X[\"time\"], errors='coerce')\n",
    "\n",
    "# Splitting Navstat\n",
    "X = pd.get_dummies(X, columns=[\"navstat\"], prefix=\"\", prefix_sep=\"\")\n",
    "X.rename(columns={\"0\": \"Under_Way\", \"1\": \"Anchored\", \"5\": \"Moored\"}, inplace=True)\n",
    "X = X.drop(columns=[\"15\"])\n",
    "\n",
    "X[\"avgSOG\"] = X[\"vesselId\"].apply(lambda x: avg.get(x))\n",
    "X[\"vesselId\"] = X[\"vesselId\"].map(vessel_dict)\n",
    "\n",
    "X_train, X_test, y_train_lat, y_test_lat = train_test_split(X, y_lat, test_size=0.2)\n",
    "\n",
    "clf_lat = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "clf_lat.fit(X_train, y_train_lat)\n",
    "y_pred_lat = clf_lat.predict(X_test)\n",
    "\n",
    "print('Random Forest Model without Feature Engineering')\n",
    "evaluate_result(y_test_lat, y_pred_lat)\n",
    "\n",
    "#print(y_test_lat.tolist()[:10])\n",
    "#print(y_pred_lat[:10])\n",
    "\n",
    "X_train, X_test, y_train_lon, y_test_lon = train_test_split(X, y_lon, test_size=0.2)\n",
    "\n",
    "#print(X_test.head())\n",
    "clf_lon = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "clf_lon.fit(X_train, y_train_lon)\n",
    "y_pred_lon = clf_lon.predict(X_test)\n",
    "\n",
    "print('Random Forest Model without Feature Engineering')\n",
    "evaluate_result(y_test_lon, y_pred_lon)\n",
    "\n",
    "#print(y_test_lon.tolist()[:10])\n",
    "#print(y_pred_lon[:10])\n",
    "\n",
    "#testRFR[\"nearPort\"] = None\n",
    "#testRFR[\"Under_Way\"] = None\n",
    "#testRFR[\"Anchored\"] = None\n",
    "#testRFR[\"Moored\"] = None\n",
    "\n",
    "test = testRFR.drop(columns=['ID', 'scaling_factor'])\n",
    "\n",
    "test[\"vesselId\"] = test[\"vesselId\"].map(vessel_dict)\n",
    "test = updateNavStat(X, test, schedulesdf)\n",
    "\n",
    "test[\"avgSOG\"] = test[\"vesselId\"].apply(lambda x: avg.get(x))\n",
    "\n",
    "#test[\"time\"] = test[\"time\"].astype(\"int64\") / 10**9\n",
    "test[\"time\"] = pd.to_numeric(test[\"time\"], errors='coerce')\n",
    "\n",
    "cols = test.columns.tolist()\n",
    "time_index = cols.index('time')\n",
    "vesselId_index = cols.index('vesselId')\n",
    "cols[time_index], cols[vesselId_index] = cols[vesselId_index], cols[time_index]\n",
    "test = test[cols]\n",
    "\n",
    "#print(testRFR.head())\n",
    "#print(test.head())\n",
    "\n",
    "lat_pred = clf_lat.predict(test)\n",
    "lon_pred = clf_lon.predict(test)\n",
    "\n",
    "#submission(sampledf, lat_pred, lon_pred, True, \"Test1\")\n",
    "#print(lat_pred)\n",
    "#print(lon_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4285cff-7e80-4689-92dd-9b091312a43e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>longitude_predicted</th>\n",
       "      <th>latitude_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-81.497905</td>\n",
       "      <td>31.146487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>120.296297</td>\n",
       "      <td>14.816909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10.809007</td>\n",
       "      <td>38.335911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>172.835428</td>\n",
       "      <td>-43.537958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-6.159701</td>\n",
       "      <td>48.550942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51734</th>\n",
       "      <td>51734</td>\n",
       "      <td>-31.899401</td>\n",
       "      <td>39.395808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51735</th>\n",
       "      <td>51735</td>\n",
       "      <td>-113.742467</td>\n",
       "      <td>27.983676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51736</th>\n",
       "      <td>51736</td>\n",
       "      <td>-113.830540</td>\n",
       "      <td>27.371876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51737</th>\n",
       "      <td>51737</td>\n",
       "      <td>17.450312</td>\n",
       "      <td>55.926400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51738</th>\n",
       "      <td>51738</td>\n",
       "      <td>-41.628289</td>\n",
       "      <td>55.486381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51739 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  longitude_predicted  latitude_predicted\n",
       "0          0           -81.497905           31.146487\n",
       "1          1           120.296297           14.816909\n",
       "2          2            10.809007           38.335911\n",
       "3          3           172.835428          -43.537958\n",
       "4          4            -6.159701           48.550942\n",
       "...      ...                  ...                 ...\n",
       "51734  51734           -31.899401           39.395808\n",
       "51735  51735          -113.742467           27.983676\n",
       "51736  51736          -113.830540           27.371876\n",
       "51737  51737            17.450312           55.926400\n",
       "51738  51738           -41.628289           55.486381\n",
       "\n",
       "[51739 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#submission(sampledf, lat_pred, lon_pred, True, \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba2329a5-2ee9-488c-bbca-d234e13b4b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCatB = traindf.copy()\n",
    "testCatB = testdf.copy()\n",
    "\n",
    "window_size = 10\n",
    "trainCatB = prev_nodes_train(trainCatB, window_size)\n",
    "testCatB = prev_nodes_test(trainCatB, testCatB, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86ae44c9-f560-4bc1-a035-e7ebb60f210f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom catboost import CatBoostRegressor\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\n\\ny_lat = trainCatB[\"latitude\"].copy()\\ny_lon = trainCatB[\"longitude\"].copy()\\n\\nX = trainCatB.drop(columns=[\\'latitude\\', \\'longitude\\', \\'etaRaw\\', \\'portId\\', \\'cog\\', \\'sog\\', \\'rot\\', \\'heading\\'])\\n\\nX[\"time\"] = pd.to_numeric(X[\"time\"], errors=\\'coerce\\')\\n\\nX = pd.get_dummies(X, columns=[\"navstat\"], prefix=\"\", prefix_sep=\"\")\\nX.rename(columns={\"0\": \"Under_Way\", \"1\": \"Anchored\", \"5\": \"Moored\"}, inplace=True)\\nX = X.drop(columns=[\"15\"])\\n\\nX[\"avgSOG\"] = X[\"vesselId\"].apply(lambda x: avg.get(x))\\nX[\"vesselId\"] = X[\"vesselId\"].map(vessel_dict)\\n\\nX_train_lat, X_test_lat, y_train_lat, y_test_lat = train_test_split(X, y_lat, test_size=0.2, random_state=42)\\nX_train_lon, X_test_lon, y_train_lon, y_test_lon = train_test_split(X, y_lon, test_size=0.2, random_state=42)\\n\\n# Initialize CatBoost Regressor with default parameters\\ncategorical_features = [\\'vesselId\\', \\'Under_Way\\', \\'Anchored\\', \\'Moored\\']\\ncatboost_lat = CatBoostRegressor(loss_function=\\'RMSE\\', cat_features=categorical_features, verbose=100)\\n\\n# Define parameter grid for hyperparameter tuning\\nparam_grid = {\\n    \\'iterations\\': [200, 500, 1000],\\n    \\'depth\\': [4, 6, 8],\\n    \\'learning_rate\\': [0.01, 0.05, 0.1],\\n    \\'l2_leaf_reg\\': [3, 5, 7],\\n    \\'random_strength\\': [1, 2, 5]\\n}\\n\\n# Hyperparameter tuning with GridSearchCV for latitude\\ngrid_search_lat = GridSearchCV(estimator=catboost_lat, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\\ngrid_search_lat.fit(X_train_lat, y_train_lat)\\nbest_params_lat = grid_search_lat.best_params_\\nprint(f\"Best parameters for latitude prediction: {best_params_lat}\")\\n\\n# Train the CatBoost model using the best parameters for latitude\\ncatboost_lat_best = CatBoostRegressor(cat_features=categorical_features, **best_params_lat, verbose=100)\\ncatboost_lat_best.fit(X_train_lat, y_train_lat)\\n\\n# Hyperparameter tuning with GridSearchCV for longitude\\ncatboost_lon = CatBoostRegressor(loss_function=\\'RMSE\\', cat_features=categorical_features, verbose=100)\\ngrid_search_lon = GridSearchCV(estimator=catboost_lon, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\\ngrid_search_lon.fit(X_train_lon, y_train_lon)\\nbest_params_lon = grid_search_lon.best_params_\\nprint(f\"Best parameters for longitude prediction: {best_params_lon}\")\\n\\n# Train the CatBoost model using the best parameters for longitude\\ncatboost_lon_best = CatBoostRegressor(cat_features=categorical_features, **best_params_lon, verbose=100)\\ncatboost_lon_best.fit(X_train_lon, y_train_lon)\\n\\n# Step 7: Evaluate the models\\ndef evaluate_result(y_test, y_pred):\\n    mse = mean_squared_error(y_test, y_pred)\\n    mae = mean_absolute_error(y_test, y_pred)\\n    r2 = r2_score(y_test, y_pred)\\n    print(f\\'Mean Squared Error (MSE): {mse:.4f}\\')\\n    print(f\\'Mean Absolute Error (MAE): {mae:.4f}\\')\\n    print(f\\'R^2 Score: {r2:.4f}\\')\\n\\n# Latitude predictions\\ny_pred_lat = catboost_lat_best.predict(X_test_lat)\\nprint(\\'CatBoost Latitude Prediction Results:\\')\\nevaluate_result(y_test_lat, y_pred_lat)\\n\\n# Longitude predictions\\ny_pred_lon = catboost_lon_best.predict(X_test_lon)\\nprint(\\'CatBoost Longitude Prediction Results:\\')\\nevaluate_result(y_test_lon, y_pred_lon)\\n\\ntest = testCatB.drop(columns=[\\'ID\\', \\'scaling_factor\\'])\\n\\ntest[\"vesselId\"] = test[\"vesselId\"].map(vessel_dict)\\ntest = updateNavStat(X, test, schedulesdf)\\n\\ntest[\"avgSOG\"] = test[\"vesselId\"].apply(lambda x: avg.get(x))\\ntest[\"time\"] = pd.to_numeric(test[\"time\"], errors=\\'coerce\\')\\n\\ncols = test.columns.tolist()\\ntime_index = cols.index(\\'time\\')\\nvesselId_index = cols.index(\\'vesselId\\')\\ncols[time_index], cols[vesselId_index] = cols[vesselId_index], cols[time_index]\\ntest = test[cols]\\n\\n# Step 8: Final predictions on test set and submission creation\\nlat_pred = catboost_lat_best.predict(test)\\nlon_pred = catboost_lon_best.predict(test)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_lat = trainCatB[\"latitude\"].copy()\n",
    "y_lon = trainCatB[\"longitude\"].copy()\n",
    "\n",
    "X = trainCatB.drop(columns=['latitude', 'longitude', 'etaRaw', 'portId', 'cog', 'sog', 'rot', 'heading'])\n",
    "\n",
    "X[\"time\"] = pd.to_numeric(X[\"time\"], errors='coerce')\n",
    "\n",
    "X = pd.get_dummies(X, columns=[\"navstat\"], prefix=\"\", prefix_sep=\"\")\n",
    "X.rename(columns={\"0\": \"Under_Way\", \"1\": \"Anchored\", \"5\": \"Moored\"}, inplace=True)\n",
    "X = X.drop(columns=[\"15\"])\n",
    "\n",
    "X[\"avgSOG\"] = X[\"vesselId\"].apply(lambda x: avg.get(x))\n",
    "X[\"vesselId\"] = X[\"vesselId\"].map(vessel_dict)\n",
    "\n",
    "X_train_lat, X_test_lat, y_train_lat, y_test_lat = train_test_split(X, y_lat, test_size=0.2, random_state=42)\n",
    "X_train_lon, X_test_lon, y_train_lon, y_test_lon = train_test_split(X, y_lon, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CatBoost Regressor with default parameters\n",
    "categorical_features = ['vesselId', 'Under_Way', 'Anchored', 'Moored']\n",
    "catboost_lat = CatBoostRegressor(loss_function='RMSE', cat_features=categorical_features, verbose=100)\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'iterations': [200, 500, 1000],\n",
    "    'depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'l2_leaf_reg': [3, 5, 7],\n",
    "    'random_strength': [1, 2, 5]\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV for latitude\n",
    "grid_search_lat = GridSearchCV(estimator=catboost_lat, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_lat.fit(X_train_lat, y_train_lat)\n",
    "best_params_lat = grid_search_lat.best_params_\n",
    "print(f\"Best parameters for latitude prediction: {best_params_lat}\")\n",
    "\n",
    "# Train the CatBoost model using the best parameters for latitude\n",
    "catboost_lat_best = CatBoostRegressor(cat_features=categorical_features, **best_params_lat, verbose=100)\n",
    "catboost_lat_best.fit(X_train_lat, y_train_lat)\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV for longitude\n",
    "catboost_lon = CatBoostRegressor(loss_function='RMSE', cat_features=categorical_features, verbose=100)\n",
    "grid_search_lon = GridSearchCV(estimator=catboost_lon, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_lon.fit(X_train_lon, y_train_lon)\n",
    "best_params_lon = grid_search_lon.best_params_\n",
    "print(f\"Best parameters for longitude prediction: {best_params_lon}\")\n",
    "\n",
    "# Train the CatBoost model using the best parameters for longitude\n",
    "catboost_lon_best = CatBoostRegressor(cat_features=categorical_features, **best_params_lon, verbose=100)\n",
    "catboost_lon_best.fit(X_train_lon, y_train_lon)\n",
    "\n",
    "# Step 7: Evaluate the models\n",
    "def evaluate_result(y_test, y_pred):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "    print(f'R^2 Score: {r2:.4f}')\n",
    "\n",
    "# Latitude predictions\n",
    "y_pred_lat = catboost_lat_best.predict(X_test_lat)\n",
    "print('CatBoost Latitude Prediction Results:')\n",
    "evaluate_result(y_test_lat, y_pred_lat)\n",
    "\n",
    "# Longitude predictions\n",
    "y_pred_lon = catboost_lon_best.predict(X_test_lon)\n",
    "print('CatBoost Longitude Prediction Results:')\n",
    "evaluate_result(y_test_lon, y_pred_lon)\n",
    "\n",
    "test = testCatB.drop(columns=['ID', 'scaling_factor'])\n",
    "\n",
    "test[\"vesselId\"] = test[\"vesselId\"].map(vessel_dict)\n",
    "test = updateNavStat(X, test, schedulesdf)\n",
    "\n",
    "test[\"avgSOG\"] = test[\"vesselId\"].apply(lambda x: avg.get(x))\n",
    "test[\"time\"] = pd.to_numeric(test[\"time\"], errors='coerce')\n",
    "\n",
    "cols = test.columns.tolist()\n",
    "time_index = cols.index('time')\n",
    "vesselId_index = cols.index('vesselId')\n",
    "cols[time_index], cols[vesselId_index] = cols[vesselId_index], cols[time_index]\n",
    "test = test[cols]\n",
    "\n",
    "# Step 8: Final predictions on test set and submission creation\n",
    "lat_pred = catboost_lat_best.predict(test)\n",
    "lon_pred = catboost_lon_best.predict(test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff26c592-cebf-44f2-b45f-219454d981d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "0:\tlearn: 20.6946594\ttotal: 238ms\tremaining: 1m 58s\n",
      "100:\tlearn: 1.1533001\ttotal: 19.5s\tremaining: 1m 17s\n",
      "200:\tlearn: 0.9825519\ttotal: 38.7s\tremaining: 57.6s\n",
      "300:\tlearn: 0.8929150\ttotal: 59.4s\tremaining: 39.2s\n",
      "400:\tlearn: 0.8286388\ttotal: 1m 18s\tremaining: 19.3s\n",
      "499:\tlearn: 0.7844239\ttotal: 1m 37s\tremaining: 0us\n",
      "Best parameters for latitude prediction: {'random_strength': 2, 'learning_rate': 0.1, 'l2_leaf_reg': 5, 'iterations': 500, 'depth': 8}\n",
      "0:\tlearn: 20.6946594\ttotal: 236ms\tremaining: 1m 57s\n",
      "100:\tlearn: 1.1533001\ttotal: 20.8s\tremaining: 1m 22s\n",
      "200:\tlearn: 0.9825519\ttotal: 39.8s\tremaining: 59.3s\n",
      "300:\tlearn: 0.8929150\ttotal: 58.8s\tremaining: 38.9s\n",
      "400:\tlearn: 0.8286388\ttotal: 1m 18s\tremaining: 19.5s\n",
      "499:\tlearn: 0.7844239\ttotal: 1m 37s\tremaining: 0us\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "0:\tlearn: 20.7364993\ttotal: 370ms\tremaining: 36.6s\n",
      "99:\tlearn: 1.4084743\ttotal: 26.4s\tremaining: 0us\n",
      "[CV] END depth=4, iterations=100, l2_leaf_reg=7, learning_rate=0.1, random_strength=1; total time=  51.6s\n",
      "0:\tlearn: 20.7082509\ttotal: 588ms\tremaining: 4m 53s\n",
      "100:\tlearn: 1.1772730\ttotal: 47.9s\tremaining: 3m 9s\n",
      "200:\tlearn: 0.9548019\ttotal: 1m 35s\tremaining: 2m 22s\n",
      "300:\tlearn: 0.8324309\ttotal: 2m 20s\tremaining: 1m 33s\n",
      "400:\tlearn: 0.7609220\ttotal: 3m 2s\tremaining: 45s\n",
      "499:\tlearn: 0.7054861\ttotal: 3m 42s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=500, l2_leaf_reg=5, learning_rate=0.1, random_strength=2; total time= 4.2min\n",
      "0:\tlearn: 21.8168511\ttotal: 537ms\tremaining: 53.1s\n",
      "99:\tlearn: 1.4133413\ttotal: 46.4s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=100, l2_leaf_reg=5, learning_rate=0.05, random_strength=2; total time= 1.4min\n",
      "0:\tlearn: 20.7488419\ttotal: 571ms\tremaining: 1m 53s\n",
      "100:\tlearn: 1.2262452\ttotal: 48.7s\tremaining: 47.7s\n",
      "199:\tlearn: 1.0054222\ttotal: 1m 24s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=200, l2_leaf_reg=7, learning_rate=0.1, random_strength=5; total time= 2.0min\n",
      "0:\tlearn: 22.7079591\ttotal: 383ms\tremaining: 38s\n",
      "99:\tlearn: 9.0956340\ttotal: 26.9s\tremaining: 0us\n",
      "[CV] END depth=4, iterations=100, l2_leaf_reg=5, learning_rate=0.01, random_strength=1; total time= 1.1min\n",
      "0:\tlearn: 21.8401707\ttotal: 376ms\tremaining: 1m 14s\n",
      "100:\tlearn: 1.6199673\ttotal: 35.4s\tremaining: 34.7s\n",
      "199:\tlearn: 1.3117956\ttotal: 1m 5s\tremaining: 0us\n",
      "[CV] END depth=6, iterations=200, l2_leaf_reg=3, learning_rate=0.05, random_strength=5; total time= 1.6min\n",
      "0:\tlearn: 62.1620898\ttotal: 275ms\tremaining: 27.2s\n",
      "99:\tlearn: 3.6253504\ttotal: 27.5s\tremaining: 0us\n",
      "[CV] END depth=4, iterations=100, l2_leaf_reg=3, learning_rate=0.1, random_strength=1; total time=  51.9s\n",
      "0:\tlearn: 62.0827974\ttotal: 604ms\tremaining: 5m 1s\n",
      "100:\tlearn: 2.9896974\ttotal: 48s\tremaining: 3m 9s\n",
      "200:\tlearn: 2.4128540\ttotal: 1m 35s\tremaining: 2m 21s\n",
      "300:\tlearn: 2.1680417\ttotal: 2m 21s\tremaining: 1m 33s\n",
      "400:\tlearn: 2.0061404\ttotal: 3m\tremaining: 44.6s\n",
      "499:\tlearn: 1.8836121\ttotal: 3m 39s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=500, l2_leaf_reg=5, learning_rate=0.1, random_strength=2; total time= 4.2min\n",
      "0:\tlearn: 65.4097384\ttotal: 495ms\tremaining: 49s\n",
      "99:\tlearn: 3.6350284\ttotal: 47.1s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=100, l2_leaf_reg=5, learning_rate=0.05, random_strength=2; total time= 1.4min\n",
      "0:\tlearn: 62.1448195\ttotal: 464ms\tremaining: 1m 32s\n",
      "100:\tlearn: 3.3003732\ttotal: 47.5s\tremaining: 46.5s\n",
      "199:\tlearn: 2.6618710\ttotal: 1m 23s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=200, l2_leaf_reg=7, learning_rate=0.1, random_strength=5; total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 20.7377942\ttotal: 472ms\tremaining: 46.8s\n",
      "99:\tlearn: 1.4217127\ttotal: 26.4s\tremaining: 0us\n",
      "[CV] END depth=4, iterations=100, l2_leaf_reg=3, learning_rate=0.1, random_strength=1; total time=  51.5s\n",
      "0:\tlearn: 20.7091162\ttotal: 612ms\tremaining: 5m 5s\n",
      "100:\tlearn: 1.1554604\ttotal: 47.2s\tremaining: 3m 6s\n",
      "200:\tlearn: 0.9557439\ttotal: 1m 34s\tremaining: 2m 20s\n",
      "300:\tlearn: 0.8492555\ttotal: 2m 21s\tremaining: 1m 33s\n",
      "400:\tlearn: 0.7703128\ttotal: 3m\tremaining: 44.6s\n",
      "499:\tlearn: 0.7133141\ttotal: 3m 41s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=500, l2_leaf_reg=5, learning_rate=0.1, random_strength=2; total time= 4.2min\n",
      "0:\tlearn: 21.8161957\ttotal: 418ms\tremaining: 41.4s\n",
      "99:\tlearn: 1.3930514\ttotal: 46.8s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=100, l2_leaf_reg=5, learning_rate=0.05, random_strength=2; total time= 1.4min\n",
      "0:\tlearn: 20.7486475\ttotal: 559ms\tremaining: 1m 51s\n",
      "100:\tlearn: 1.2242090\ttotal: 49.8s\tremaining: 48.8s\n",
      "199:\tlearn: 0.9980543\ttotal: 1m 29s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=200, l2_leaf_reg=7, learning_rate=0.1, random_strength=5; total time= 2.0min\n",
      "0:\tlearn: 22.7058985\ttotal: 241ms\tremaining: 23.8s\n",
      "99:\tlearn: 9.0997520\ttotal: 26.8s\tremaining: 0us\n",
      "[CV] END depth=4, iterations=100, l2_leaf_reg=5, learning_rate=0.01, random_strength=1; total time= 1.0min\n",
      "0:\tlearn: 21.8384619\ttotal: 425ms\tremaining: 1m 24s\n",
      "100:\tlearn: 1.5913840\ttotal: 34.3s\tremaining: 33.7s\n",
      "199:\tlearn: 1.3042887\ttotal: 1m 9s\tremaining: 0us\n",
      "[CV] END depth=6, iterations=200, l2_leaf_reg=3, learning_rate=0.05, random_strength=5; total time= 1.7min\n",
      "0:\tlearn: 62.1650386\ttotal: 240ms\tremaining: 23.8s\n",
      "99:\tlearn: 3.6847353\ttotal: 27.2s\tremaining: 0us\n",
      "[CV] END depth=4, iterations=100, l2_leaf_reg=7, learning_rate=0.1, random_strength=1; total time=  51.2s\n",
      "0:\tlearn: 62.1528008\ttotal: 414ms\tremaining: 3m 26s\n",
      "100:\tlearn: 3.6060640\ttotal: 34.5s\tremaining: 2m 16s\n",
      "200:\tlearn: 2.9672752\ttotal: 1m 8s\tremaining: 1m 42s\n",
      "300:\tlearn: 2.6305726\ttotal: 1m 42s\tremaining: 1m 7s\n",
      "400:\tlearn: 2.4444621\ttotal: 2m 15s\tremaining: 33.5s\n",
      "499:\tlearn: 2.2949801\ttotal: 2m 49s\tremaining: 0us\n",
      "[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.1, random_strength=5; total time= 3.3min\n",
      "0:\tlearn: 65.4161793\ttotal: 588ms\tremaining: 4m 53s\n",
      "100:\tlearn: 3.5893520\ttotal: 37.5s\tremaining: 2m 28s\n",
      "200:\tlearn: 2.9555975\ttotal: 1m 19s\tremaining: 1m 58s\n",
      "300:\tlearn: 2.6464666\ttotal: 1m 56s\tremaining: 1m 17s\n",
      "400:\tlearn: 2.4290731\ttotal: 2m 35s\tremaining: 38.3s\n",
      "499:\tlearn: 2.2832827\ttotal: 3m 20s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=500, l2_leaf_reg=7, learning_rate=0.05, random_strength=2; total time= 4.0min\n",
      "0:\tlearn: 62.0471103\ttotal: 190ms\tremaining: 1m 34s\n",
      "100:\tlearn: 3.1655622\ttotal: 26.7s\tremaining: 1m 45s\n",
      "200:\tlearn: 2.6806287\ttotal: 55.8s\tremaining: 1m 22s\n",
      "300:\tlearn: 2.4297085\ttotal: 1m 20s\tremaining: 53.1s\n",
      "400:\tlearn: 2.2769695\ttotal: 1m 50s\tremaining: 27.4s\n",
      "499:\tlearn: 2.1682263\ttotal: 2m 24s\tremaining: 0us\n",
      "[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.1, random_strength=1; total time= 2.9min\n",
      "0:\tlearn: 62.0228839\ttotal: 226ms\tremaining: 1m 52s\n",
      "100:\tlearn: 2.9690351\ttotal: 19.8s\tremaining: 1m 18s\n",
      "200:\tlearn: 2.5324303\ttotal: 38.8s\tremaining: 57.8s\n",
      "300:\tlearn: 2.2902722\ttotal: 57.7s\tremaining: 38.1s\n",
      "400:\tlearn: 2.1492155\ttotal: 1m 17s\tremaining: 19.2s\n",
      "499:\tlearn: 2.0407442\ttotal: 1m 36s\tremaining: 0us\n",
      "Best parameters for longitude prediction: {'random_strength': 2, 'learning_rate': 0.1, 'l2_leaf_reg': 5, 'iterations': 500, 'depth': 8}\n",
      "0:\tlearn: 62.0228839\ttotal: 232ms\tremaining: 1m 55s\n",
      "100:\tlearn: 2.9690351\ttotal: 18.9s\tremaining: 1m 14s\n",
      "200:\tlearn: 2.5324303\ttotal: 38.3s\tremaining: 57s\n",
      "300:\tlearn: 2.2902722\ttotal: 57.2s\tremaining: 37.8s\n",
      "400:\tlearn: 2.1492155\ttotal: 1m 17s\tremaining: 19.1s\n",
      "499:\tlearn: 2.0407442\ttotal: 1m 36s\tremaining: 0us\n",
      "CatBoost Latitude Prediction Results:\n",
      "Mean Squared Error (MSE): 0.9529\n",
      "Mean Absolute Error (MAE): 0.1948\n",
      "R^2 Score: 0.9982\n",
      "CatBoost Longitude Prediction Results:\n",
      "Mean Squared Error (MSE): 7.8387\n",
      "Mean Absolute Error (MAE): 0.4701\n",
      "R^2 Score: 0.9983\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_lat = trainCatB[\"latitude\"].copy()\n",
    "y_lon = trainCatB[\"longitude\"].copy()\n",
    "\n",
    "X = trainCatB.drop(columns=['latitude', 'longitude', 'etaRaw', 'portId', 'cog', 'sog', 'rot', 'heading'])\n",
    "\n",
    "X[\"time\"] = pd.to_numeric(X[\"time\"], errors='coerce')\n",
    "\n",
    "X = pd.get_dummies(X, columns=[\"navstat\"], prefix=\"\", prefix_sep=\"\")\n",
    "X.rename(columns={\"0\": \"Under_Way\", \"1\": \"Anchored\", \"5\": \"Moored\"}, inplace=True)\n",
    "X = X.drop(columns=[\"15\"])\n",
    "\n",
    "X[\"avgSOG\"] = X[\"vesselId\"].apply(lambda x: avg.get(x))\n",
    "X[\"vesselId\"] = X[\"vesselId\"].map(vessel_dict)\n",
    "\n",
    "test = testCatB.drop(columns=['ID', 'scaling_factor'])\n",
    "\n",
    "test[\"vesselId\"] = test[\"vesselId\"].map(vessel_dict)\n",
    "test = updateNavStat(X, test, schedulesdf)\n",
    "\n",
    "test[\"avgSOG\"] = test[\"vesselId\"].apply(lambda x: avg.get(x))\n",
    "test[\"time\"] = pd.to_numeric(test[\"time\"], errors='coerce')\n",
    "\n",
    "cols = test.columns.tolist()\n",
    "time_index = cols.index('time')\n",
    "vesselId_index = cols.index('vesselId')\n",
    "cols[time_index], cols[vesselId_index] = cols[vesselId_index], cols[time_index]\n",
    "test = test[cols]\n",
    "test = test.drop(columns=[\"vesselId\"])\n",
    "\n",
    "X = X.drop(columns=[\"vesselId\"])\n",
    "\n",
    "X_train_lat, X_test_lat, y_train_lat, y_test_lat = train_test_split(X, y_lat, test_size=0.2, random_state=42)\n",
    "X_train_lon, X_test_lon, y_train_lon, y_test_lon = train_test_split(X, y_lon, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CatBoost Regressor with default parameters\n",
    "#categorical_features = ['vesselId', 'Under_Way', 'Anchored', 'Moored']\n",
    "categorical_features = ['Under_Way', 'Anchored', 'Moored']\n",
    "catboost_lat = CatBoostRegressor(loss_function='RMSE', cat_features=categorical_features, verbose=100)\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_distributions = {\n",
    "    'iterations': [100, 200, 500],\n",
    "    'depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'l2_leaf_reg': [3, 5, 7],\n",
    "    'random_strength': [1, 2, 5]\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV for latitude\n",
    "random_search_lat = RandomizedSearchCV(estimator=catboost_lat, param_distributions=param_distributions, n_iter=10, cv=2, n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search_lat.fit(X_train_lat, y_train_lat)\n",
    "best_params_lat = random_search_lat.best_params_\n",
    "print(f\"Best parameters for latitude prediction: {best_params_lat}\")\n",
    "\n",
    "# Train the CatBoost model using the best parameters for latitude\n",
    "catboost_lat_best = CatBoostRegressor(cat_features=categorical_features, **best_params_lat, verbose=100)\n",
    "catboost_lat_best.fit(X_train_lat, y_train_lat)\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV for longitude\n",
    "catboost_lon = CatBoostRegressor(loss_function='RMSE', cat_features=categorical_features, verbose=100)\n",
    "random_search_lon = RandomizedSearchCV(estimator=catboost_lon, param_distributions=param_distributions, n_iter=10, cv=2, n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search_lon.fit(X_train_lon, y_train_lon)\n",
    "best_params_lon = random_search_lon.best_params_\n",
    "print(f\"Best parameters for longitude prediction: {best_params_lon}\")\n",
    "\n",
    "# Train the CatBoost model using the best parameters for longitude\n",
    "catboost_lon_best = CatBoostRegressor(cat_features=categorical_features, **best_params_lon, verbose=100)\n",
    "catboost_lon_best.fit(X_train_lon, y_train_lon)\n",
    "\n",
    "# Step 7: Evaluate the models\n",
    "def evaluate_result(y_test, y_pred):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "    print(f'R^2 Score: {r2:.4f}')\n",
    "\n",
    "# Latitude predictions\n",
    "y_pred_lat = catboost_lat_best.predict(X_test_lat)\n",
    "print('CatBoost Latitude Prediction Results:')\n",
    "evaluate_result(y_test_lat, y_pred_lat)\n",
    "\n",
    "# Longitude predictions\n",
    "y_pred_lon = catboost_lon_best.predict(X_test_lon)\n",
    "print('CatBoost Longitude Prediction Results:')\n",
    "evaluate_result(y_test_lon, y_pred_lon)\n",
    "\n",
    "# Step 8: Final predictions on test set and submission creation\n",
    "lat_pred = catboost_lat_best.predict(test)\n",
    "lon_pred = catboost_lon_best.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddcc7714-7e51-462f-90b2-51af8f364442",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>longitude_predicted</th>\n",
       "      <th>latitude_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-79.068688</td>\n",
       "      <td>32.087130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>121.174651</td>\n",
       "      <td>15.907451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11.279773</td>\n",
       "      <td>38.566537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>172.589341</td>\n",
       "      <td>-42.895073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-5.705859</td>\n",
       "      <td>48.927629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51734</th>\n",
       "      <td>51734</td>\n",
       "      <td>-60.111333</td>\n",
       "      <td>29.827106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51735</th>\n",
       "      <td>51735</td>\n",
       "      <td>-108.212500</td>\n",
       "      <td>23.140989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51736</th>\n",
       "      <td>51736</td>\n",
       "      <td>-110.211305</td>\n",
       "      <td>21.182255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51737</th>\n",
       "      <td>51737</td>\n",
       "      <td>13.870155</td>\n",
       "      <td>49.820389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51738</th>\n",
       "      <td>51738</td>\n",
       "      <td>11.526218</td>\n",
       "      <td>54.655605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51739 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  longitude_predicted  latitude_predicted\n",
       "0          0           -79.068688           32.087130\n",
       "1          1           121.174651           15.907451\n",
       "2          2            11.279773           38.566537\n",
       "3          3           172.589341          -42.895073\n",
       "4          4            -5.705859           48.927629\n",
       "...      ...                  ...                 ...\n",
       "51734  51734           -60.111333           29.827106\n",
       "51735  51735          -108.212500           23.140989\n",
       "51736  51736          -110.211305           21.182255\n",
       "51737  51737            13.870155           49.820389\n",
       "51738  51738            11.526218           54.655605\n",
       "\n",
       "[51739 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 20.7361354\ttotal: 363ms\tremaining: 35.9s\n",
      "99:\tlearn: 1.3931875\ttotal: 26.4s\tremaining: 0us\n",
      "[CV] END depth=4, iterations=100, l2_leaf_reg=3, learning_rate=0.1, random_strength=1; total time=  50.0s\n",
      "0:\tlearn: 20.7553570\ttotal: 290ms\tremaining: 2m 24s\n",
      "100:\tlearn: 1.3385595\ttotal: 35.8s\tremaining: 2m 21s\n",
      "200:\tlearn: 1.1129108\ttotal: 1m 10s\tremaining: 1m 44s\n",
      "300:\tlearn: 1.0045030\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "400:\tlearn: 0.9267248\ttotal: 2m 18s\tremaining: 34.1s\n",
      "499:\tlearn: 0.8712098\ttotal: 2m 51s\tremaining: 0us\n",
      "[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.1, random_strength=5; total time= 3.3min\n",
      "0:\tlearn: 21.8165725\ttotal: 516ms\tremaining: 4m 17s\n",
      "100:\tlearn: 1.3974188\ttotal: 39.9s\tremaining: 2m 37s\n",
      "200:\tlearn: 1.1465745\ttotal: 1m 21s\tremaining: 2m 1s\n",
      "300:\tlearn: 1.0287128\ttotal: 2m 2s\tremaining: 1m 21s\n",
      "400:\tlearn: 0.9589968\ttotal: 2m 42s\tremaining: 40.2s\n",
      "499:\tlearn: 0.9009091\ttotal: 3m 29s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=500, l2_leaf_reg=7, learning_rate=0.05, random_strength=2; total time= 4.1min\n",
      "0:\tlearn: 20.7024046\ttotal: 276ms\tremaining: 2m 17s\n",
      "100:\tlearn: 1.2399200\ttotal: 28.8s\tremaining: 1m 53s\n",
      "200:\tlearn: 1.0553700\ttotal: 59.3s\tremaining: 1m 28s\n",
      "300:\tlearn: 0.9592596\ttotal: 1m 24s\tremaining: 56s\n",
      "400:\tlearn: 0.8867828\ttotal: 1m 59s\tremaining: 29.5s\n",
      "499:\tlearn: 0.8348383\ttotal: 2m 32s\tremaining: 0us\n",
      "[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.1, random_strength=1; total time= 3.0min\n",
      "0:\tlearn: 62.1629421\ttotal: 216ms\tremaining: 21.4s\n",
      "99:\tlearn: 3.6570220\ttotal: 27.6s\tremaining: 0us\n",
      "[CV] END depth=4, iterations=100, l2_leaf_reg=7, learning_rate=0.1, random_strength=1; total time=  50.7s\n",
      "0:\tlearn: 62.3148041\ttotal: 373ms\tremaining: 3m 6s\n",
      "100:\tlearn: 3.5563294\ttotal: 34.8s\tremaining: 2m 17s\n",
      "200:\tlearn: 2.9103462\ttotal: 1m 9s\tremaining: 1m 43s\n",
      "300:\tlearn: 2.6251671\ttotal: 1m 43s\tremaining: 1m 8s\n",
      "400:\tlearn: 2.4449304\ttotal: 2m 17s\tremaining: 34s\n",
      "499:\tlearn: 2.3106647\ttotal: 2m 51s\tremaining: 0us\n",
      "[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.1, random_strength=5; total time= 3.3min\n",
      "0:\tlearn: 65.4108751\ttotal: 696ms\tremaining: 5m 47s\n",
      "100:\tlearn: 3.6189768\ttotal: 36.9s\tremaining: 2m 25s\n",
      "200:\tlearn: 2.9508493\ttotal: 1m 19s\tremaining: 1m 58s\n",
      "300:\tlearn: 2.6346691\ttotal: 1m 56s\tremaining: 1m 17s\n",
      "400:\tlearn: 2.4180328\ttotal: 2m 36s\tremaining: 38.7s\n",
      "499:\tlearn: 2.2849689\ttotal: 3m 22s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=500, l2_leaf_reg=7, learning_rate=0.05, random_strength=2; total time= 4.0min\n",
      "0:\tlearn: 62.0462042\ttotal: 298ms\tremaining: 2m 28s\n",
      "100:\tlearn: 3.2521916\ttotal: 26.6s\tremaining: 1m 44s\n",
      "200:\tlearn: 2.7324552\ttotal: 56.2s\tremaining: 1m 23s\n",
      "300:\tlearn: 2.4716149\ttotal: 1m 21s\tremaining: 54s\n",
      "400:\tlearn: 2.3041601\ttotal: 1m 53s\tremaining: 28s\n",
      "499:\tlearn: 2.1839760\ttotal: 2m 26s\tremaining: 0us\n",
      "[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.1, random_strength=1; total time= 2.9min\n",
      "0:\tlearn: 20.7382068\ttotal: 323ms\tremaining: 32s\n",
      "99:\tlearn: 1.4238314\ttotal: 26.1s\tremaining: 0us\n",
      "[CV] END depth=4, iterations=100, l2_leaf_reg=7, learning_rate=0.1, random_strength=1; total time=  50.5s\n",
      "0:\tlearn: 20.7566971\ttotal: 424ms\tremaining: 3m 31s\n",
      "100:\tlearn: 1.3553493\ttotal: 35.7s\tremaining: 2m 20s\n",
      "200:\tlearn: 1.1078824\ttotal: 1m 9s\tremaining: 1m 43s\n",
      "300:\tlearn: 0.9965555\ttotal: 1m 44s\tremaining: 1m 9s\n",
      "400:\tlearn: 0.9242240\ttotal: 2m 19s\tremaining: 34.4s\n",
      "499:\tlearn: 0.8720259\ttotal: 2m 52s\tremaining: 0us\n",
      "[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.1, random_strength=5; total time= 3.4min\n",
      "0:\tlearn: 21.8172568\ttotal: 616ms\tremaining: 5m 7s\n",
      "100:\tlearn: 1.4117529\ttotal: 38.7s\tremaining: 2m 33s\n",
      "200:\tlearn: 1.1520928\ttotal: 1m 21s\tremaining: 2m 1s\n",
      "300:\tlearn: 1.0302953\ttotal: 2m\tremaining: 1m 19s\n",
      "400:\tlearn: 0.9448133\ttotal: 2m 41s\tremaining: 39.8s\n",
      "499:\tlearn: 0.8826867\ttotal: 3m 27s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=500, l2_leaf_reg=7, learning_rate=0.05, random_strength=2; total time= 4.1min\n",
      "0:\tlearn: 20.7046453\ttotal: 325ms\tremaining: 2m 42s\n",
      "100:\tlearn: 1.2610481\ttotal: 28.2s\tremaining: 1m 51s\n",
      "200:\tlearn: 1.0553346\ttotal: 57.5s\tremaining: 1m 25s\n",
      "300:\tlearn: 0.9561159\ttotal: 1m 22s\tremaining: 54.7s\n",
      "400:\tlearn: 0.8897567\ttotal: 1m 56s\tremaining: 28.8s\n",
      "499:\tlearn: 0.8414805\ttotal: 2m 29s\tremaining: 0us\n",
      "[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.1, random_strength=1; total time= 2.9min\n",
      "0:\tlearn: 62.1641751\ttotal: 226ms\tremaining: 22.4s\n",
      "99:\tlearn: 3.6535164\ttotal: 28s\tremaining: 0us\n",
      "[CV] END depth=4, iterations=100, l2_leaf_reg=3, learning_rate=0.1, random_strength=1; total time=  51.7s\n",
      "0:\tlearn: 62.0839073\ttotal: 547ms\tremaining: 4m 32s\n",
      "100:\tlearn: 3.0008759\ttotal: 47.3s\tremaining: 3m 6s\n",
      "200:\tlearn: 2.4361205\ttotal: 1m 33s\tremaining: 2m 18s\n",
      "300:\tlearn: 2.1464179\ttotal: 2m 17s\tremaining: 1m 30s\n",
      "400:\tlearn: 1.9694329\ttotal: 2m 58s\tremaining: 44s\n",
      "499:\tlearn: 1.8462912\ttotal: 3m 36s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=500, l2_leaf_reg=5, learning_rate=0.1, random_strength=2; total time= 4.1min\n",
      "0:\tlearn: 65.4151948\ttotal: 562ms\tremaining: 55.6s\n",
      "99:\tlearn: 3.5745928\ttotal: 48.5s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=100, l2_leaf_reg=5, learning_rate=0.05, random_strength=2; total time= 1.4min\n",
      "0:\tlearn: 62.2572147\ttotal: 570ms\tremaining: 1m 53s\n",
      "100:\tlearn: 3.2776109\ttotal: 47.5s\tremaining: 46.5s\n",
      "199:\tlearn: 2.6605576\ttotal: 1m 25s\tremaining: 0us\n",
      "[CV] END depth=8, iterations=200, l2_leaf_reg=7, learning_rate=0.1, random_strength=5; total time= 2.0min\n",
      "0:\tlearn: 68.0918575\ttotal: 188ms\tremaining: 18.6s\n",
      "99:\tlearn: 27.1807053\ttotal: 24.7s\tremaining: 0us\n",
      "[CV] END depth=4, iterations=100, l2_leaf_reg=5, learning_rate=0.01, random_strength=1; total time= 1.0min\n",
      "0:\tlearn: 65.5267080\ttotal: 287ms\tremaining: 57.1s\n",
      "100:\tlearn: 4.2646764\ttotal: 32.4s\tremaining: 31.7s\n",
      "199:\tlearn: 3.4671810\ttotal: 1m 6s\tremaining: 0us\n",
      "[CV] END depth=6, iterations=200, l2_leaf_reg=3, learning_rate=0.05, random_strength=5; total time= 1.6min\n",
      "0:\tlearn: 68.0829528\ttotal: 335ms\tremaining: 33.1s\n",
      "99:\tlearn: 27.1863328\ttotal: 24.5s\tremaining: 0us\n",
      "[CV] END depth=4, iterations=100, l2_leaf_reg=5, learning_rate=0.01, random_strength=1; total time= 1.0min\n",
      "0:\tlearn: 65.4430728\ttotal: 265ms\tremaining: 52.8s\n",
      "100:\tlearn: 4.3322844\ttotal: 35.9s\tremaining: 35.2s\n",
      "199:\tlearn: 3.4632375\ttotal: 1m 2s\tremaining: 0us\n",
      "[CV] END depth=6, iterations=200, l2_leaf_reg=3, learning_rate=0.05, random_strength=5; total time= 1.5min\n"
     ]
    }
   ],
   "source": [
    "submission(sampledf, lat_pred, lon_pred, True, \"catboost_prediction_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564effc-1330-4652-ad3e-264eee295991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
